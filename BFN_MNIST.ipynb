{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/home/krisztina/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/')\n",
    "import wandb\n",
    "from trainer import DiscreteBFNTrainer\n",
    "wandb.init(project=\"bfn\")\n",
    "trainer = DiscreteBFNTrainer(wandb_project_name='bfn', device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krisztina/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: nan\n",
      "./bfn_model_checkpoint.pth\n",
      "Epoch 1/10, Batch 2/1856\n",
      "Epoch 1/10, Loss: 0.5761930346488953\n",
      "Epoch 1/10, Batch 3/1856\n",
      "Epoch 1/10, Loss: 0.49328941106796265\n",
      "Epoch 1/10, Batch 4/1856\n",
      "Epoch 1/10, Loss: 0.42215219140052795\n",
      "Epoch 1/10, Batch 5/1856\n",
      "Epoch 1/10, Loss: 0.3588221073150635\n",
      "Epoch 1/10, Batch 6/1856\n",
      "Epoch 1/10, Loss: 0.31949061155319214\n",
      "Epoch 1/10, Batch 7/1856\n",
      "Epoch 1/10, Loss: 0.28888824582099915\n",
      "Epoch 1/10, Batch 8/1856\n",
      "Epoch 1/10, Loss: 0.2636065185070038\n",
      "Epoch 1/10, Batch 9/1856\n",
      "Epoch 1/10, Loss: 0.24114491045475006\n",
      "Epoch 1/10, Batch 10/1856\n",
      "Epoch 1/10, Loss: 0.22435910999774933\n",
      "Epoch 1/10, Batch 11/1856\n",
      "Epoch 1/10, Loss: 0.21024885773658752\n",
      "Epoch 1/10, Batch 12/1856\n",
      "Epoch 1/10, Loss: 0.19807936251163483\n",
      "Epoch 1/10, Batch 13/1856\n",
      "Epoch 1/10, Loss: 0.18810391426086426\n",
      "Epoch 1/10, Batch 14/1856\n",
      "Epoch 1/10, Loss: 0.17841733992099762\n",
      "Epoch 1/10, Batch 15/1856\n",
      "Epoch 1/10, Loss: 0.17031726241111755\n",
      "Epoch 1/10, Batch 16/1856\n",
      "Epoch 1/10, Loss: 0.16351468861103058\n",
      "Epoch 1/10, Batch 17/1856\n",
      "Epoch 1/10, Loss: 0.15775640308856964\n",
      "Epoch 1/10, Batch 18/1856\n",
      "Epoch 1/10, Loss: 0.1520788073539734\n",
      "Epoch 1/10, Batch 19/1856\n",
      "Epoch 1/10, Loss: 0.14711475372314453\n",
      "Epoch 1/10, Batch 20/1856\n",
      "Epoch 1/10, Loss: 0.14244472980499268\n",
      "Epoch 1/10, Batch 21/1856\n",
      "Epoch 1/10, Loss: 0.13838037848472595\n",
      "Epoch 1/10, Batch 22/1856\n",
      "Epoch 1/10, Loss: 0.13444530963897705\n",
      "Epoch 1/10, Batch 23/1856\n",
      "Epoch 1/10, Loss: 0.1311996877193451\n",
      "Epoch 1/10, Batch 24/1856\n",
      "Epoch 1/10, Loss: 0.1282200664281845\n",
      "Epoch 1/10, Batch 25/1856\n",
      "Epoch 1/10, Loss: 0.12535715103149414\n",
      "Epoch 1/10, Batch 26/1856\n",
      "Epoch 1/10, Loss: 0.1228722408413887\n",
      "Epoch 1/10, Batch 27/1856\n",
      "Epoch 1/10, Loss: 0.12071822583675385\n",
      "Epoch 1/10, Batch 28/1856\n",
      "Epoch 1/10, Loss: 0.11880653351545334\n",
      "Epoch 1/10, Batch 29/1856\n",
      "Epoch 1/10, Loss: 0.11650741100311279\n",
      "Epoch 1/10, Batch 30/1856\n",
      "Epoch 1/10, Loss: 0.11444346606731415\n",
      "Epoch 1/10, Batch 31/1856\n",
      "Epoch 1/10, Loss: 0.11257175356149673\n",
      "Epoch 1/10, Batch 32/1856\n",
      "Epoch 1/10, Loss: 0.11097312718629837\n",
      "Epoch 1/10, Batch 33/1856\n",
      "Epoch 1/10, Loss: 0.10962602496147156\n",
      "Epoch 1/10, Batch 34/1856\n",
      "Epoch 1/10, Loss: 0.10803793370723724\n",
      "Epoch 1/10, Batch 35/1856\n",
      "Epoch 1/10, Loss: 0.10650230944156647\n",
      "Epoch 1/10, Batch 36/1856\n",
      "Epoch 1/10, Loss: 0.10526372492313385\n",
      "Epoch 1/10, Batch 37/1856\n",
      "Epoch 1/10, Loss: 0.1038699820637703\n",
      "Epoch 1/10, Batch 38/1856\n",
      "Epoch 1/10, Loss: 0.10281781852245331\n",
      "Epoch 1/10, Batch 39/1856\n",
      "Epoch 1/10, Loss: 0.10180438309907913\n",
      "Epoch 1/10, Batch 40/1856\n",
      "Epoch 1/10, Loss: 0.10059598833322525\n",
      "Epoch 1/10, Batch 41/1856\n",
      "Epoch 1/10, Loss: 0.09939886629581451\n",
      "Epoch 1/10, Batch 42/1856\n",
      "Epoch 1/10, Loss: 0.09840881824493408\n",
      "Epoch 1/10, Batch 43/1856\n",
      "Epoch 1/10, Loss: 0.0976187139749527\n",
      "Epoch 1/10, Batch 44/1856\n",
      "Epoch 1/10, Loss: 0.09650566428899765\n",
      "Epoch 1/10, Batch 45/1856\n",
      "Epoch 1/10, Loss: 0.09558162093162537\n",
      "Epoch 1/10, Batch 46/1856\n",
      "Epoch 1/10, Loss: 0.0947866439819336\n",
      "Epoch 1/10, Batch 47/1856\n",
      "Epoch 1/10, Loss: 0.09390254318714142\n",
      "Epoch 1/10, Batch 48/1856\n",
      "Epoch 1/10, Loss: 0.09309542179107666\n",
      "Epoch 1/10, Batch 49/1856\n",
      "Epoch 1/10, Loss: 0.09232159703969955\n",
      "Epoch 1/10, Batch 50/1856\n",
      "Epoch 1/10, Loss: 0.09188047796487808\n",
      "Epoch 1/10, Batch 51/1856\n",
      "Epoch 1/10, Loss: 0.0911373421549797\n",
      "Epoch 1/10, Batch 52/1856\n",
      "Epoch 1/10, Loss: 0.09047721326351166\n",
      "Epoch 1/10, Batch 53/1856\n",
      "Epoch 1/10, Loss: 0.0899425819516182\n",
      "Epoch 1/10, Batch 54/1856\n",
      "Epoch 1/10, Loss: 0.0892232283949852\n",
      "Epoch 1/10, Batch 55/1856\n",
      "Epoch 1/10, Loss: 0.0885419249534607\n",
      "Epoch 1/10, Batch 56/1856\n",
      "Epoch 1/10, Loss: 0.08791610598564148\n",
      "Epoch 1/10, Batch 57/1856\n",
      "Epoch 1/10, Loss: 0.08724647015333176\n",
      "Epoch 1/10, Batch 58/1856\n",
      "Epoch 1/10, Loss: 0.08661555498838425\n",
      "Epoch 1/10, Batch 59/1856\n",
      "Epoch 1/10, Loss: 0.0863041877746582\n",
      "Epoch 1/10, Batch 60/1856\n",
      "Epoch 1/10, Loss: 0.08594249933958054\n",
      "Epoch 1/10, Batch 61/1856\n",
      "Epoch 1/10, Loss: 0.08549162745475769\n",
      "Epoch 1/10, Batch 62/1856\n",
      "Epoch 1/10, Loss: 0.0850229486823082\n",
      "Epoch 1/10, Batch 63/1856\n",
      "Epoch 1/10, Loss: 0.08460734784603119\n",
      "Epoch 1/10, Batch 64/1856\n",
      "Epoch 1/10, Loss: 0.08427166193723679\n",
      "Epoch 1/10, Batch 65/1856\n",
      "Epoch 1/10, Loss: 0.08381082862615585\n",
      "Epoch 1/10, Batch 66/1856\n",
      "Epoch 1/10, Loss: 0.0833854004740715\n",
      "Epoch 1/10, Batch 67/1856\n",
      "Epoch 1/10, Loss: 0.08302904665470123\n",
      "Epoch 1/10, Batch 68/1856\n",
      "Epoch 1/10, Loss: 0.08259003609418869\n",
      "Epoch 1/10, Batch 69/1856\n",
      "Epoch 1/10, Loss: 0.08213213831186295\n",
      "Epoch 1/10, Batch 70/1856\n",
      "Epoch 1/10, Loss: 0.08174672722816467\n",
      "Epoch 1/10, Batch 71/1856\n",
      "Epoch 1/10, Loss: 0.08120600134134293\n",
      "Epoch 1/10, Batch 72/1856\n",
      "Epoch 1/10, Loss: 0.08090440183877945\n",
      "Epoch 1/10, Batch 73/1856\n",
      "Epoch 1/10, Loss: 0.08055483549833298\n",
      "Epoch 1/10, Batch 74/1856\n",
      "Epoch 1/10, Loss: 0.0802680030465126\n",
      "Epoch 1/10, Batch 75/1856\n",
      "Epoch 1/10, Loss: 0.07987353950738907\n",
      "Epoch 1/10, Batch 76/1856\n",
      "Epoch 1/10, Loss: 0.07956385612487793\n",
      "Epoch 1/10, Batch 77/1856\n",
      "Epoch 1/10, Loss: 0.07926974445581436\n",
      "Epoch 1/10, Batch 78/1856\n",
      "Epoch 1/10, Loss: 0.07890156656503677\n",
      "Epoch 1/10, Batch 79/1856\n",
      "Epoch 1/10, Loss: 0.07857010513544083\n",
      "Epoch 1/10, Batch 80/1856\n",
      "Epoch 1/10, Loss: 0.07837966084480286\n",
      "Epoch 1/10, Batch 81/1856\n",
      "Epoch 1/10, Loss: 0.07801250368356705\n",
      "Epoch 1/10, Batch 82/1856\n",
      "Epoch 1/10, Loss: 0.07763606309890747\n",
      "Epoch 1/10, Batch 83/1856\n",
      "Epoch 1/10, Loss: 0.07737015187740326\n",
      "Epoch 1/10, Batch 84/1856\n",
      "Epoch 1/10, Loss: 0.07715743780136108\n",
      "Epoch 1/10, Batch 85/1856\n",
      "Epoch 1/10, Loss: 0.0769171193242073\n",
      "Epoch 1/10, Batch 86/1856\n",
      "Epoch 1/10, Loss: 0.07667777687311172\n",
      "Epoch 1/10, Batch 87/1856\n",
      "Epoch 1/10, Loss: 0.0763712003827095\n",
      "Epoch 1/10, Batch 88/1856\n",
      "Epoch 1/10, Loss: 0.07610197365283966\n",
      "Epoch 1/10, Batch 89/1856\n",
      "Epoch 1/10, Loss: 0.0758611187338829\n",
      "Epoch 1/10, Batch 90/1856\n",
      "Epoch 1/10, Loss: 0.07556696236133575\n",
      "Epoch 1/10, Batch 91/1856\n",
      "Epoch 1/10, Loss: 0.07536821067333221\n",
      "Epoch 1/10, Batch 92/1856\n",
      "Epoch 1/10, Loss: 0.07513962686061859\n",
      "Epoch 1/10, Batch 93/1856\n",
      "Epoch 1/10, Loss: 0.07496128231287003\n",
      "Epoch 1/10, Batch 94/1856\n",
      "Epoch 1/10, Loss: 0.07475902885198593\n",
      "Epoch 1/10, Batch 95/1856\n",
      "Epoch 1/10, Loss: 0.07451849430799484\n",
      "Epoch 1/10, Batch 96/1856\n",
      "Epoch 1/10, Loss: 0.07431864738464355\n",
      "Epoch 1/10, Batch 97/1856\n",
      "Epoch 1/10, Loss: 0.07412771135568619\n",
      "Epoch 1/10, Batch 98/1856\n",
      "Epoch 1/10, Loss: 0.07385954260826111\n",
      "Epoch 1/10, Batch 99/1856\n",
      "Epoch 1/10, Loss: 0.07368085533380508\n",
      "Epoch 1/10, Batch 100/1856\n",
      "Epoch 1/10, Loss: 0.07342217117547989\n",
      "Epoch 1/10, Batch 101/1856\n",
      "Epoch 1/10, Loss: 0.07330012321472168\n",
      "./bfn_model_checkpoint.pth\n",
      "Epoch 1/10, Batch 102/1856\n",
      "Epoch 1/10, Loss: 0.07315721362829208\n",
      "Epoch 1/10, Batch 103/1856\n",
      "Epoch 1/10, Loss: 0.0728590339422226\n",
      "Epoch 1/10, Batch 104/1856\n",
      "Epoch 1/10, Loss: 0.07273844629526138\n",
      "Epoch 1/10, Batch 105/1856\n",
      "Epoch 1/10, Loss: 0.07252898067235947\n",
      "Epoch 1/10, Batch 106/1856\n",
      "Epoch 1/10, Loss: 0.07237072288990021\n",
      "Epoch 1/10, Batch 107/1856\n",
      "Epoch 1/10, Loss: 0.07214252650737762\n",
      "Epoch 1/10, Batch 108/1856\n",
      "Epoch 1/10, Loss: 0.07197831571102142\n",
      "Epoch 1/10, Batch 109/1856\n",
      "Epoch 1/10, Loss: 0.07183395326137543\n",
      "Epoch 1/10, Batch 110/1856\n",
      "Epoch 1/10, Loss: 0.07160642743110657\n",
      "Epoch 1/10, Batch 111/1856\n",
      "Epoch 1/10, Loss: 0.07146050035953522\n",
      "Epoch 1/10, Batch 112/1856\n",
      "Epoch 1/10, Loss: 0.07122749835252762\n",
      "Epoch 1/10, Batch 113/1856\n",
      "Epoch 1/10, Loss: 0.0710061565041542\n",
      "Epoch 1/10, Batch 114/1856\n",
      "Epoch 1/10, Loss: 0.07082150876522064\n",
      "Epoch 1/10, Batch 115/1856\n",
      "Epoch 1/10, Loss: 0.07058975100517273\n",
      "Epoch 1/10, Batch 116/1856\n",
      "Epoch 1/10, Loss: 0.0704357922077179\n",
      "Epoch 1/10, Batch 117/1856\n",
      "Epoch 1/10, Loss: 0.0702795460820198\n",
      "Epoch 1/10, Batch 118/1856\n",
      "Epoch 1/10, Loss: 0.07011265307664871\n",
      "Epoch 1/10, Batch 119/1856\n",
      "Epoch 1/10, Loss: 0.06995268911123276\n",
      "Epoch 1/10, Batch 120/1856\n",
      "Epoch 1/10, Loss: 0.06977856904268265\n",
      "Epoch 1/10, Batch 121/1856\n",
      "Epoch 1/10, Loss: 0.06960956007242203\n",
      "Epoch 1/10, Batch 122/1856\n",
      "Epoch 1/10, Loss: 0.0694652572274208\n",
      "Epoch 1/10, Batch 123/1856\n",
      "Epoch 1/10, Loss: 0.06935657560825348\n",
      "Epoch 1/10, Batch 124/1856\n",
      "Epoch 1/10, Loss: 0.06927048414945602\n",
      "Epoch 1/10, Batch 125/1856\n",
      "Epoch 1/10, Loss: 0.069142185151577\n",
      "Epoch 1/10, Batch 126/1856\n",
      "Epoch 1/10, Loss: 0.06891892850399017\n",
      "Epoch 1/10, Batch 127/1856\n",
      "Epoch 1/10, Loss: 0.06874022632837296\n",
      "Epoch 1/10, Batch 128/1856\n",
      "Epoch 1/10, Loss: 0.06863933801651001\n",
      "Epoch 1/10, Batch 129/1856\n",
      "Epoch 1/10, Loss: 0.06845703721046448\n",
      "Epoch 1/10, Batch 130/1856\n",
      "Epoch 1/10, Loss: 0.0683026984333992\n",
      "Epoch 1/10, Batch 131/1856\n",
      "Epoch 1/10, Loss: 0.0681609958410263\n",
      "Epoch 1/10, Batch 132/1856\n",
      "Epoch 1/10, Loss: 0.06801770627498627\n",
      "Epoch 1/10, Batch 133/1856\n",
      "Epoch 1/10, Loss: 0.06792100518941879\n",
      "Epoch 1/10, Batch 134/1856\n",
      "Epoch 1/10, Loss: 0.0677638053894043\n",
      "Epoch 1/10, Batch 135/1856\n",
      "Epoch 1/10, Loss: 0.06764593720436096\n",
      "Epoch 1/10, Batch 136/1856\n",
      "Epoch 1/10, Loss: 0.0675475224852562\n",
      "Epoch 1/10, Batch 137/1856\n",
      "Epoch 1/10, Loss: 0.06745275110006332\n",
      "Epoch 1/10, Batch 138/1856\n",
      "Epoch 1/10, Loss: 0.06735069304704666\n",
      "Epoch 1/10, Batch 139/1856\n",
      "Epoch 1/10, Loss: 0.06724054366350174\n",
      "Epoch 1/10, Batch 140/1856\n",
      "Epoch 1/10, Loss: 0.06716088950634003\n",
      "Epoch 1/10, Batch 141/1856\n",
      "Epoch 1/10, Loss: 0.0670684352517128\n",
      "Epoch 1/10, Batch 142/1856\n",
      "Epoch 1/10, Loss: 0.06697036325931549\n",
      "Epoch 1/10, Batch 143/1856\n",
      "Epoch 1/10, Loss: 0.06683409959077835\n",
      "Epoch 1/10, Batch 144/1856\n",
      "Epoch 1/10, Loss: 0.06671509146690369\n",
      "Epoch 1/10, Batch 145/1856\n",
      "Epoch 1/10, Loss: 0.06664426624774933\n",
      "Epoch 1/10, Batch 146/1856\n",
      "Epoch 1/10, Loss: 0.06651121377944946\n",
      "Epoch 1/10, Batch 147/1856\n",
      "Epoch 1/10, Loss: 0.06641404330730438\n",
      "Epoch 1/10, Batch 148/1856\n",
      "Epoch 1/10, Loss: 0.06630373746156693\n",
      "Epoch 1/10, Batch 149/1856\n",
      "Epoch 1/10, Loss: 0.06618531048297882\n",
      "Epoch 1/10, Batch 150/1856\n",
      "Epoch 1/10, Loss: 0.06604450196027756\n",
      "Epoch 1/10, Batch 151/1856\n",
      "Epoch 1/10, Loss: 0.06590818613767624\n",
      "Epoch 1/10, Batch 152/1856\n",
      "Epoch 1/10, Loss: 0.06583219766616821\n",
      "Epoch 1/10, Batch 153/1856\n",
      "Epoch 1/10, Loss: 0.06574998050928116\n",
      "Epoch 1/10, Batch 154/1856\n",
      "Epoch 1/10, Loss: 0.06566588580608368\n",
      "Epoch 1/10, Batch 155/1856\n",
      "Epoch 1/10, Loss: 0.06556084007024765\n",
      "Epoch 1/10, Batch 156/1856\n",
      "Epoch 1/10, Loss: 0.06546902656555176\n",
      "Epoch 1/10, Batch 157/1856\n",
      "Epoch 1/10, Loss: 0.06538058072328568\n",
      "Epoch 1/10, Batch 158/1856\n",
      "Epoch 1/10, Loss: 0.06526494026184082\n",
      "Epoch 1/10, Batch 159/1856\n",
      "Epoch 1/10, Loss: 0.0651358962059021\n",
      "Epoch 1/10, Batch 160/1856\n",
      "Epoch 1/10, Loss: 0.06509176641702652\n",
      "Epoch 1/10, Batch 161/1856\n",
      "Epoch 1/10, Loss: 0.06502097100019455\n",
      "Epoch 1/10, Batch 162/1856\n",
      "Epoch 1/10, Loss: 0.06491436809301376\n",
      "Epoch 1/10, Batch 163/1856\n",
      "Epoch 1/10, Loss: 0.06477399915456772\n",
      "Epoch 1/10, Batch 164/1856\n",
      "Epoch 1/10, Loss: 0.0646781325340271\n",
      "Epoch 1/10, Batch 165/1856\n",
      "Epoch 1/10, Loss: 0.06455785781145096\n",
      "Epoch 1/10, Batch 166/1856\n",
      "Epoch 1/10, Loss: 0.06447706371545792\n",
      "Epoch 1/10, Batch 167/1856\n",
      "Epoch 1/10, Loss: 0.06436153501272202\n",
      "Epoch 1/10, Batch 168/1856\n",
      "Epoch 1/10, Loss: 0.06424753367900848\n",
      "Epoch 1/10, Batch 169/1856\n",
      "Epoch 1/10, Loss: 0.06420890241861343\n",
      "Epoch 1/10, Batch 170/1856\n",
      "Epoch 1/10, Loss: 0.06411617249250412\n",
      "Epoch 1/10, Batch 171/1856\n",
      "Epoch 1/10, Loss: 0.06400272995233536\n",
      "Epoch 1/10, Batch 172/1856\n",
      "Epoch 1/10, Loss: 0.06391563266515732\n",
      "Epoch 1/10, Batch 173/1856\n",
      "Epoch 1/10, Loss: 0.06382773071527481\n",
      "Epoch 1/10, Batch 174/1856\n",
      "Epoch 1/10, Loss: 0.06372179090976715\n",
      "Epoch 1/10, Batch 175/1856\n",
      "Epoch 1/10, Loss: 0.06361294537782669\n",
      "Epoch 1/10, Batch 176/1856\n",
      "Epoch 1/10, Loss: 0.06354761123657227\n",
      "Epoch 1/10, Batch 177/1856\n",
      "Epoch 1/10, Loss: 0.06345842033624649\n",
      "Epoch 1/10, Batch 178/1856\n",
      "Epoch 1/10, Loss: 0.0634240061044693\n",
      "Epoch 1/10, Batch 179/1856\n",
      "Epoch 1/10, Loss: 0.06335926055908203\n",
      "Epoch 1/10, Batch 180/1856\n",
      "Epoch 1/10, Loss: 0.06326373666524887\n",
      "Epoch 1/10, Batch 181/1856\n",
      "Epoch 1/10, Loss: 0.06319676339626312\n",
      "Epoch 1/10, Batch 182/1856\n",
      "Epoch 1/10, Loss: 0.06310240924358368\n",
      "Epoch 1/10, Batch 183/1856\n",
      "Epoch 1/10, Loss: 0.06302443891763687\n",
      "Epoch 1/10, Batch 184/1856\n",
      "Epoch 1/10, Loss: 0.06292952597141266\n",
      "Epoch 1/10, Batch 185/1856\n",
      "Epoch 1/10, Loss: 0.06283600628376007\n",
      "Epoch 1/10, Batch 186/1856\n",
      "Epoch 1/10, Loss: 0.06279260665178299\n",
      "Epoch 1/10, Batch 187/1856\n",
      "Epoch 1/10, Loss: 0.06274835765361786\n",
      "Epoch 1/10, Batch 188/1856\n",
      "Epoch 1/10, Loss: 0.06269299983978271\n",
      "Epoch 1/10, Batch 189/1856\n",
      "Epoch 1/10, Loss: 0.06262969970703125\n",
      "Epoch 1/10, Batch 190/1856\n",
      "Epoch 1/10, Loss: 0.06255338340997696\n",
      "Epoch 1/10, Batch 191/1856\n",
      "Epoch 1/10, Loss: 0.06247267499566078\n",
      "Epoch 1/10, Batch 192/1856\n",
      "Epoch 1/10, Loss: 0.06242844834923744\n",
      "Epoch 1/10, Batch 193/1856\n",
      "Epoch 1/10, Loss: 0.062401991337537766\n",
      "Epoch 1/10, Batch 194/1856\n",
      "Epoch 1/10, Loss: 0.06234484165906906\n",
      "Epoch 1/10, Batch 195/1856\n",
      "Epoch 1/10, Loss: 0.06228060647845268\n",
      "Epoch 1/10, Batch 196/1856\n",
      "Epoch 1/10, Loss: 0.0622079111635685\n",
      "Epoch 1/10, Batch 197/1856\n",
      "Epoch 1/10, Loss: 0.06210926175117493\n",
      "Epoch 1/10, Batch 198/1856\n",
      "Epoch 1/10, Loss: 0.06202971935272217\n",
      "Epoch 1/10, Batch 199/1856\n",
      "Epoch 1/10, Loss: 0.06196369603276253\n",
      "Epoch 1/10, Batch 200/1856\n",
      "Epoch 1/10, Loss: 0.061888113617897034\n",
      "Epoch 1/10, Batch 201/1856\n",
      "Epoch 1/10, Loss: 0.06182609871029854\n",
      "./bfn_model_checkpoint.pth\n",
      "Epoch 1/10, Batch 202/1856\n",
      "Epoch 1/10, Loss: 0.06175410747528076\n",
      "Epoch 1/10, Batch 203/1856\n",
      "Epoch 1/10, Loss: 0.06169668957591057\n",
      "Epoch 1/10, Batch 204/1856\n",
      "Epoch 1/10, Loss: 0.06163499876856804\n",
      "Epoch 1/10, Batch 205/1856\n",
      "Epoch 1/10, Loss: 0.06156850978732109\n",
      "Epoch 1/10, Batch 206/1856\n",
      "Epoch 1/10, Loss: 0.06146224960684776\n",
      "Epoch 1/10, Batch 207/1856\n",
      "Epoch 1/10, Loss: 0.061399586498737335\n",
      "Epoch 1/10, Batch 208/1856\n",
      "Epoch 1/10, Loss: 0.06130792200565338\n",
      "Epoch 1/10, Batch 209/1856\n",
      "Epoch 1/10, Loss: 0.061266183853149414\n",
      "Epoch 1/10, Batch 210/1856\n",
      "Epoch 1/10, Loss: 0.06116057559847832\n",
      "Epoch 1/10, Batch 211/1856\n",
      "Epoch 1/10, Loss: 0.06109124422073364\n",
      "Epoch 1/10, Batch 212/1856\n",
      "Epoch 1/10, Loss: 0.06102137267589569\n",
      "Epoch 1/10, Batch 213/1856\n",
      "Epoch 1/10, Loss: 0.060976702719926834\n",
      "Epoch 1/10, Batch 214/1856\n",
      "Epoch 1/10, Loss: 0.060947079211473465\n",
      "Epoch 1/10, Batch 215/1856\n",
      "Epoch 1/10, Loss: 0.06090215593576431\n",
      "Epoch 1/10, Batch 216/1856\n",
      "Epoch 1/10, Loss: 0.06083809956908226\n",
      "Epoch 1/10, Batch 217/1856\n",
      "Epoch 1/10, Loss: 0.06079305708408356\n",
      "Epoch 1/10, Batch 218/1856\n",
      "Epoch 1/10, Loss: 0.060689862817525864\n",
      "Epoch 1/10, Batch 219/1856\n",
      "Epoch 1/10, Loss: 0.06061835587024689\n",
      "Epoch 1/10, Batch 220/1856\n",
      "Epoch 1/10, Loss: 0.060572411864995956\n",
      "Epoch 1/10, Batch 221/1856\n",
      "Epoch 1/10, Loss: 0.060489341616630554\n",
      "Epoch 1/10, Batch 222/1856\n",
      "Epoch 1/10, Loss: 0.06044718250632286\n",
      "Epoch 1/10, Batch 223/1856\n",
      "Epoch 1/10, Loss: 0.06038559228181839\n",
      "Epoch 1/10, Batch 224/1856\n",
      "Epoch 1/10, Loss: 0.060351770371198654\n",
      "Epoch 1/10, Batch 225/1856\n",
      "Epoch 1/10, Loss: 0.060319554060697556\n",
      "Epoch 1/10, Batch 226/1856\n",
      "Epoch 1/10, Loss: 0.060274120420217514\n",
      "Epoch 1/10, Batch 227/1856\n",
      "Epoch 1/10, Loss: 0.06021227315068245\n",
      "Epoch 1/10, Batch 228/1856\n",
      "Epoch 1/10, Loss: 0.06011942774057388\n",
      "Epoch 1/10, Batch 229/1856\n",
      "Epoch 1/10, Loss: 0.060035958886146545\n",
      "Epoch 1/10, Batch 230/1856\n",
      "Epoch 1/10, Loss: 0.05998774245381355\n",
      "Epoch 1/10, Batch 231/1856\n",
      "Epoch 1/10, Loss: 0.05994206294417381\n",
      "Epoch 1/10, Batch 232/1856\n",
      "Epoch 1/10, Loss: 0.05988067388534546\n",
      "Epoch 1/10, Batch 233/1856\n",
      "Epoch 1/10, Loss: 0.05981651693582535\n",
      "Epoch 1/10, Batch 234/1856\n",
      "Epoch 1/10, Loss: 0.059758394956588745\n",
      "Epoch 1/10, Batch 235/1856\n",
      "Epoch 1/10, Loss: 0.05971621349453926\n",
      "Epoch 1/10, Batch 236/1856\n",
      "Epoch 1/10, Loss: 0.05966835096478462\n",
      "Epoch 1/10, Batch 237/1856\n",
      "Epoch 1/10, Loss: 0.05959755554795265\n",
      "Epoch 1/10, Batch 238/1856\n",
      "Epoch 1/10, Loss: 0.05952725559473038\n",
      "Epoch 1/10, Batch 239/1856\n",
      "Epoch 1/10, Loss: 0.05947333574295044\n",
      "Epoch 1/10, Batch 240/1856\n",
      "Epoch 1/10, Loss: 0.05941649526357651\n",
      "Epoch 1/10, Batch 241/1856\n",
      "Epoch 1/10, Loss: 0.05934558063745499\n",
      "Epoch 1/10, Batch 242/1856\n",
      "Epoch 1/10, Loss: 0.059265926480293274\n",
      "Epoch 1/10, Batch 243/1856\n",
      "Epoch 1/10, Loss: 0.05922246351838112\n",
      "Epoch 1/10, Batch 244/1856\n",
      "Epoch 1/10, Loss: 0.05915973708033562\n",
      "Epoch 1/10, Batch 245/1856\n",
      "Epoch 1/10, Loss: 0.05910685285925865\n",
      "Epoch 1/10, Batch 246/1856\n",
      "Epoch 1/10, Loss: 0.05907980725169182\n",
      "Epoch 1/10, Batch 247/1856\n",
      "Epoch 1/10, Loss: 0.0590248666703701\n",
      "Epoch 1/10, Batch 248/1856\n",
      "Epoch 1/10, Loss: 0.05898047238588333\n",
      "Epoch 1/10, Batch 249/1856\n",
      "Epoch 1/10, Loss: 0.05894274264574051\n",
      "Epoch 1/10, Batch 250/1856\n",
      "Epoch 1/10, Loss: 0.058873847126960754\n",
      "Epoch 1/10, Batch 251/1856\n",
      "Epoch 1/10, Loss: 0.058822035789489746\n",
      "Epoch 1/10, Batch 252/1856\n",
      "Epoch 1/10, Loss: 0.05877837911248207\n",
      "Epoch 1/10, Batch 253/1856\n",
      "Epoch 1/10, Loss: 0.05873943865299225\n",
      "Epoch 1/10, Batch 254/1856\n",
      "Epoch 1/10, Loss: 0.05872349068522453\n",
      "Epoch 1/10, Batch 255/1856\n",
      "Epoch 1/10, Loss: 0.05868283659219742\n",
      "Epoch 1/10, Batch 256/1856\n",
      "Epoch 1/10, Loss: 0.058639366179704666\n",
      "Epoch 1/10, Batch 257/1856\n",
      "Epoch 1/10, Loss: 0.05860401317477226\n",
      "Epoch 1/10, Batch 258/1856\n",
      "Epoch 1/10, Loss: 0.05857006460428238\n",
      "Epoch 1/10, Batch 259/1856\n",
      "Epoch 1/10, Loss: 0.05853094533085823\n",
      "Epoch 1/10, Batch 260/1856\n",
      "Epoch 1/10, Loss: 0.058472324162721634\n",
      "Epoch 1/10, Batch 261/1856\n",
      "Epoch 1/10, Loss: 0.05845189839601517\n",
      "Epoch 1/10, Batch 262/1856\n",
      "Epoch 1/10, Loss: 0.058410268276929855\n",
      "Epoch 1/10, Batch 263/1856\n",
      "Epoch 1/10, Loss: 0.058337289839982986\n",
      "Epoch 1/10, Batch 264/1856\n",
      "Epoch 1/10, Loss: 0.05828910693526268\n",
      "Epoch 1/10, Batch 265/1856\n",
      "Epoch 1/10, Loss: 0.058241039514541626\n",
      "Epoch 1/10, Batch 266/1856\n",
      "Epoch 1/10, Loss: 0.05818980559706688\n",
      "Epoch 1/10, Batch 267/1856\n",
      "Epoch 1/10, Loss: 0.05814157426357269\n",
      "Epoch 1/10, Batch 268/1856\n",
      "Epoch 1/10, Loss: 0.05810791626572609\n",
      "Epoch 1/10, Batch 269/1856\n",
      "Epoch 1/10, Loss: 0.058081794530153275\n",
      "Epoch 1/10, Batch 270/1856\n",
      "Epoch 1/10, Loss: 0.0580533929169178\n",
      "Epoch 1/10, Batch 271/1856\n",
      "Epoch 1/10, Loss: 0.057999931275844574\n",
      "Epoch 1/10, Batch 272/1856\n",
      "Epoch 1/10, Loss: 0.057943522930145264\n",
      "Epoch 1/10, Batch 273/1856\n",
      "Epoch 1/10, Loss: 0.05790577828884125\n",
      "Epoch 1/10, Batch 274/1856\n",
      "Epoch 1/10, Loss: 0.0578889474272728\n",
      "Epoch 1/10, Batch 275/1856\n",
      "Epoch 1/10, Loss: 0.057888224720954895\n",
      "Epoch 1/10, Batch 276/1856\n",
      "Epoch 1/10, Loss: 0.05783830210566521\n",
      "Epoch 1/10, Batch 277/1856\n",
      "Epoch 1/10, Loss: 0.057794876396656036\n",
      "Epoch 1/10, Batch 278/1856\n",
      "Epoch 1/10, Loss: 0.057753752917051315\n",
      "Epoch 1/10, Batch 279/1856\n",
      "Epoch 1/10, Loss: 0.057721253484487534\n",
      "Epoch 1/10, Batch 280/1856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/trainer.py:137\u001b[0m, in \u001b[0;36mDiscreteBFNTrainer.train\u001b[0;34m(self, num_epochs, validation_interval_epoch, sampling_interval_step, save_checkpoint_interval_step, clip_grad, n_test_batches)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/bfn/bfn_discrete.py:237\u001b[0m, in \u001b[0;36mBFNDiscrete.continuous_time_loss_for_discrete_data\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    234\u001b[0m theta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Calculate p_output\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m p_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscrete_output_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Calculate coninuous Loss\u001b[39;00m\n\u001b[1;32m    240\u001b[0m e_x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(x, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK) \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/bfn/bfn_discrete.py:204\u001b[0m, in \u001b[0;36mBFNDiscrete.discrete_output_distribution\u001b[0;34m(self, theta, t)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03mtheta (torch.Tensor): Input tensor of shape (B, D, K).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m                      If K>2, tensor shape is (B, D, K).\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m t \u001b[38;5;241m=\u001b[39m right_pad_dims_to(t, theta) \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m net_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    207\u001b[0m     po_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(net_output) \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/bfn/bfn_discrete.py:188\u001b[0m, in \u001b[0;36mBFNDiscrete.forward\u001b[0;34m(self, theta, t)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# theta = (theta * 2) - 1 # rescale to [-1, 1] to have distribution centered around 0\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# theta = theta.view(theta.shape[0], -1) # (B, D*K)\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# input_ = torch.cat((theta, t.unsqueeze(-1)), dim=-1) # (B, D*K + 1)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# output = self.model(input_) # (B, D*K)\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# output =  output.view(output.shape[0], self.D, -1) # (B, D, K)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m net_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_to_net_inputs((theta, t)) \u001b[38;5;66;03m# if K = 2: (B, D, K) -> (B, D, 1) scaled to [-1,1]\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, D, K)\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/models/unet_improved.py:737\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, data, t)\u001b[0m\n\u001b[1;32m    735\u001b[0m h \u001b[38;5;241m=\u001b[39m x_perm\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dtype)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks:\n\u001b[0;32m--> 737\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    739\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_block(h, emb)\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLMI/Advanced_ML/BayesianFlowNetworks/models/unet_improved.py:296\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m    294\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, emb)\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLMI/RL/.conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets.dataset_utils import get_image_grid_from_tensor\n",
    "model = trainer.bfn_model\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples and priors\n",
    "samples = model.sample(n_steps=20, device='cpu')\n",
    "samples = samples.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.transpose(1, 3)\n",
    "image_grid = get_image_grid_from_tensor(samples)\n",
    "# Convert samples and priors to numpy arrays\n",
    "image_grid = image_grid.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape, image_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "wandb.init(project=\"bayesian_flow\")\n",
    "image_grid = np.transpose(image_grid, (2, 1, 0))\n",
    "print(image_grid.shape)\n",
    "images = wandb.Image(image_grid, caption=\"MNIST - Sampled Images from BFN\")\n",
    "wandb.log({\"image_samples\": images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Image.fromarray(image_grid)\n",
    "plt.imshow(image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
